{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Custom decision tree regressor\n",
    "from statmodels.decisiontrees import DecisionTreeRegressor\n",
    "#Custom random forest regressor\n",
    "from statmodels.random_forest import RandomForestRegressor\n",
    "#Custom gradient boosting regressor\n",
    "from statmodels.gradientboosting import GradientBoostTreeRegressor\n",
    "from statmodels.regression import LassoRegression\n",
    "from statmodels.regression.utils.metrics import r2_score, mean_squared_error\n",
    "from statmodels.regression.utils.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"energydata_complete.csv\")\n",
    "data = data.set_index(\"date\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.iloc[:, 1:].values\n",
    "Y = data.iloc[:, 0].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=.2, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.1 Lasso regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statmodels.regression.regression.LassoRegression at 0x20a9d94d4f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_transformed = sc.fit_transform(X_train)\n",
    "X_test_transformed = sc.transform(X_test) \n",
    "model_LassoRegression = LassoRegression(n_iter=10000, lr=1e-4, alpha=0.1)\n",
    "model_LassoRegression.fit(X_train_transformed, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression - Root Mean Squared error: 97.5554\n",
      "Lasso Regression - R-Squared: 0.097\n"
     ]
    }
   ],
   "source": [
    "y_hat_LassoReg = model_LassoRegression.predict(X_train_transformed)\n",
    "print('Lasso Regression - Root Mean Squared error:',\n",
    "      round(mean_squared_error(y_true=Y_train, y_pred=y_hat_LassoReg, squared= False), 4))\n",
    "print('Lasso Regression - R-Squared:',\n",
    "      round(r2_score(y_true=Y_train, y_pred= y_hat_LassoReg), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Root Mean Squared error of Lasso regression model is 97.56. As the lower the RMSE, the better the model, this model does not fit the train dataset well. This model has the R-Squared value of 0.097, which means 9.7% of the variability observed in the target variable is explained by the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.2 Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeRegressor(max_depth=5, min_samples_split=2)\n",
    "model_tree.fit(X_train, Y_train.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree Regression - Root Mean Squared error: 98.8429\n",
      "Decision tree Regression - R-Squared: 0.073\n"
     ]
    }
   ],
   "source": [
    "y_hat_tree = model_tree.predict(X_train)\n",
    "print('Decision tree Regression - Root Mean Squared error:', round(mean_squared_error(Y_train, y_hat_tree, False), 4))\n",
    "print('Decision tree Regression - R-Squared:', round(r2_score(Y_train,y_hat_tree), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Root Mean Squared error of Decision tree model is 98.84. This model does not fit the train dataset well. This model has the R-Squared value of 0.073, which means 7.3% of the variability observed in the target variable is explained by the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.3 Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestRegressor(n_trees = 5, max_depth = 6, min_samples_split = 5)\n",
    "model_RF.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression - Root Mean Squared error: 93.0082\n",
      "Random Forest Regression - R-Squared: 0.1792\n"
     ]
    }
   ],
   "source": [
    "y_hat_RF = model_RF.predict(X_train)\n",
    "print('Random Forest Regression - Root Mean Squared error:', round(mean_squared_error(Y_train, y_hat_RF, False), 4))\n",
    "print('Random Forest Regression - R-Squared:', round(r2_score(Y_train, y_hat_RF), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Root Mean Squared error of Random Forest model is 93.0082. This model does not fit the train dataset well. This model has the R-Squared value of 0.1792, which means 17.92% of the variability observed in the target variable is explained by the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.4 GradientBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GradiantBoost = GradientBoostTreeRegressor(n_elements=10, learning_rate=0.01)\n",
    "    \n",
    "#fit the model\n",
    "model_GradiantBoost.fit(np.array(X_train), Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Regression - Root Mean Squared error: 134.8229\n",
      "Gradient Boost Regression - R-Squared: -0.7247\n"
     ]
    }
   ],
   "source": [
    "y_hat_GB = model_GradiantBoost.predict(X_train)\n",
    "print('Gradient Boost Regression - Root Mean Squared error:', round(mean_squared_error(Y_train,y_hat_GB, False),4))\n",
    "print('Gradient Boost Regression - R-Squared:', round(r2_score(Y_train, y_hat_GB),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Root Mean Squared error of GradientBoost model is 134.8229. This model does not fit the train dataset well. This model has the R-Squared value of -0.7247, which this model fits the dataset poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "The Random Forest model has the lowest RMSE, which means it predicts the training data best. And it also has the highest R-square value which means it explains the most variability of the training dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.1 Lasso regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression - Root Mean Squared error: 97.1062\n",
      "Lasso Regression - R-Squared: 0.093\n"
     ]
    }
   ],
   "source": [
    "y_hat_test_LassoReg = model_LassoRegression.predict(X_test_transformed)\n",
    "print('Lasso Regression - Root Mean Squared error:',\n",
    "      round(mean_squared_error(Y_test, y_hat_test_LassoReg, False), 4))\n",
    "print('Lasso Regression - R-Squared:',\n",
    "      round(r2_score(Y_test, y_hat_test_LassoReg), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lasso regression has RMSE of 97.1062, which is similiar to the RMSE of training dataset.\n",
    "2. The model has R-Squared value of 0.0927. 9.27% of the variability observed in the target variable is explained by the regression model.\n",
    "3. Compared to training set, the RMSE is similiar between train and test set. Therefore the model is not overfitting.\n",
    "4. R-Squared is very close to zero, which means the model does not fit the dataset well and it is underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.2 Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree Regression - Root Mean Squared error: 100.3224\n",
      "Decision tree Regression - R-Squared: 0.0319\n"
     ]
    }
   ],
   "source": [
    "y_hat_test_tree = model_tree.predict(X_test)\n",
    "print('Decision tree Regression - Root Mean Squared error:', round(mean_squared_error(Y_test, y_hat_test_tree, False), 4))\n",
    "print('Decision tree Regression - R-Squared:', round(r2_score(Y_test, y_hat_test_tree), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decision tree has RMSE of 100.3224.\n",
    "2. R-Squared value of 0.0319. And 3.2% of the variability observed in the target variable is explained by the model.\n",
    "3. The RMSE of the test set is similiar to the train set, which means the model is not overfitting.\n",
    "4. R-Squared is close to 0 which means the model is underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.3 Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression - Root Mean Squared error: 93.7816\n",
      "Random Forest Regression - R-Squared: 0.154\n"
     ]
    }
   ],
   "source": [
    "y_hat_test_RF = model_RF.predict(X_test)\n",
    "print('Random Forest Regression - Root Mean Squared error:', round(mean_squared_error(Y_test, y_hat_test_RF.reshape(-1), False), 4))\n",
    "print('Random Forest Regression - R-Squared:', round(r2_score(Y_test, y_hat_test_RF.reshape(-1)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Forest model has RMSE of 93.7816.\n",
    "2. The model has R-Squared value of 0.154. This model predict well on the test dataset. And 15.4% of the variability observed in the target variable is explained by the regression model.\n",
    "3. The R-Squared value of this model is the highest among the four models. It is the best model for this dataset.\n",
    "4. The RMSE of the test set is similiar to the train set, which means the model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.4 GradientBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Regression - Root Mean Squared error: 133.8199\n",
      "Gradient Boost Regression - R-Squared: -0.7225\n"
     ]
    }
   ],
   "source": [
    "y_hat_test_GB = model_GradiantBoost.predict(X_test)\n",
    "print('Gradient Boost Regression - Root Mean Squared error:', round(mean_squared_error(Y_test, y_hat_test_GB, False), 4))\n",
    "print('Gradient Boost Regression - R-Squared:', round(r2_score(Y_test,  y_hat_test_GB),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GradientBoost model has RMSE of 133.8199\n",
    "2. The model has R-Squared value of -0.7225. This means it does not predict the test dataset well.\n",
    "3. Negative R-Squared value suggest this model is underfitting.\n",
    "4. The RMSE of the test set is similiar to the train set, which means the model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c \n",
    "Do you see any bias and variance issues? How do you interpret each model output? (4x3=12 points)\n",
    "\n",
    "## c.1 Lasso Regression\n",
    "The R-Squared value of this model is close to 0 which indicates it has high bias and the test RMSE does not increase which suggest the model has low variance.\n",
    "\n",
    "## c.2 Decision Tree Regression\n",
    "The R-Squared value of this model is close to 0 which indicates it has high bias and the test RMSE does not increase which suggest the model has low variance.\n",
    "\n",
    "## c.3 Random Forest\n",
    "The R-Squared value of this model is close to 1 which indicates it has low bias. And the test RMSE does not increase which suggest the model has low variance.\n",
    "\n",
    "## c.4 Gradient Boost\n",
    "This model has high bias because the R-Squared value is negative and it does not make enough assumptions to the dataset. And the similiar RMSE values between train and test sets suggests it have low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important features\n",
    "The following functions calculate the importance of features by mutating each feature and calculate the change of r square score. If a feature changes and the r square value also changes significiantly, then the feature is important to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_after_permutation(model, X, y, col_idx):\n",
    "    X_mutated = X.copy()\n",
    "    X_mutated[:, col_idx] = np.random.permutation(\n",
    "        X_mutated[:, col_idx])\n",
    "    permutated_score = r2_score(model.predict(X_mutated), y.reshape(-1))\n",
    "    return permutated_score\n",
    "\n",
    "\n",
    "def get_feature_importance(model, X, y, col_idx):\n",
    "    baseline_score_train = r2_score(model.predict(X), y.reshape(-1))\n",
    "    permutated_score_train = get_score_after_permutation(model, X, y, col_idx)\n",
    "    feature_importance = baseline_score_train - permutated_score_train\n",
    "    return feature_importance\n",
    "\n",
    "\n",
    "def calculate_All_Feature_Importance(model, X, y):\n",
    "    list_Feature_Importance = []\n",
    "    for col_Index in range(X.shape[1]):\n",
    "        list_Feature_Importance.append(\n",
    "            get_feature_importance(model, X, y, col_Index))\n",
    "    return list_Feature_Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d.1 Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appliances have the importance of -1.2566896409026782\n",
      "lights have the importance of -0.2852172387786993\n",
      "T1 have the importance of -8.368445184303553\n",
      "RH_1 have the importance of 3.2194094972822995\n",
      "T2 have the importance of 2.868195260783665\n",
      "RH_2 have the importance of 2.4599037037742733\n",
      "T3 have the importance of -4.197751688079762\n",
      "RH_3 have the importance of -1.439936815446032\n",
      "T4 have the importance of -0.9256331992126974\n",
      "RH_4 have the importance of -1.3862833843381281\n",
      "T5 have the importance of -0.3401224462060526\n",
      "RH_5 have the importance of 7.960023511015507\n",
      "T6 have the importance of 13.518239450763673\n",
      "RH_6 have the importance of -1.2784814639617252\n",
      "T7 have the importance of 4.1195217652591225\n",
      "RH_7 have the importance of 0.35759985375270276\n",
      "T8 have the importance of 9.121771550969882\n",
      "RH_8 have the importance of -1.6388840009409904\n",
      "T9 have the importance of 3.0933022151537415\n",
      "RH_9 have the importance of 2.2962267386232043\n",
      "T_out have the importance of -0.3375256462123133\n",
      "Press_mm_hg have the importance of 28.744125224373548\n",
      "RH_out have the importance of 0.9940699437162834\n",
      "Windspeed have the importance of -0.45844399329220664\n",
      "Visibility have the importance of -0.41679633636975666\n",
      "Tdewpoint have the importance of 0.05884715342986624\n",
      "rv1 have the importance of 0.10882309023635628\n"
     ]
    }
   ],
   "source": [
    "importances = calculate_All_Feature_Importance(model_LassoRegression, X_train, Y_train)\n",
    "for label, score in zip(data.columns, importances):\n",
    "    print(f\"{label} have the importance of {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press_mm_hg has the highest score of importance, and therefore it is the most important feature of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d.2 Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appliances have the importance of 0.0\n",
      "lights have the importance of 0.0\n",
      "T1 have the importance of 0.0\n",
      "RH_1 have the importance of 0.0\n",
      "T2 have the importance of -10.446803996482123\n",
      "RH_2 have the importance of 0.0\n",
      "T3 have the importance of -0.46076493610095426\n",
      "RH_3 have the importance of 0.0\n",
      "T4 have the importance of 0.0\n",
      "RH_4 have the importance of 0.0\n",
      "T5 have the importance of 0.0\n",
      "RH_5 have the importance of -0.8233568281804668\n",
      "T6 have the importance of 1.0103223305481048\n",
      "RH_6 have the importance of 0.0\n",
      "T7 have the importance of 3.732086436757019\n",
      "RH_7 have the importance of 0.6152929075156628\n",
      "T8 have the importance of 0.14277960515826038\n",
      "RH_8 have the importance of 0.4288490860232006\n",
      "T9 have the importance of 1.4553354824045925\n",
      "RH_9 have the importance of 0.0\n",
      "T_out have the importance of 0.0\n",
      "Press_mm_hg have the importance of -3.718543740120552\n",
      "RH_out have the importance of 0.0\n",
      "Windspeed have the importance of 0.0\n",
      "Visibility have the importance of 4.0411236706652325\n",
      "Tdewpoint have the importance of 0.0\n",
      "rv1 have the importance of 0.0\n"
     ]
    }
   ],
   "source": [
    "importances = calculate_All_Feature_Importance(model_tree, X_train, Y_train)\n",
    "for label, score in zip(data.columns, importances):\n",
    "    print(f\"{label} have the importance of {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RH_1 has the highest score followed by Appliances and T7. Therefore RH_1, Appliances and T7 are the most important feature of Decision Tree Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appliances have the importance of 1.9308648073433492\n",
      "lights have the importance of -0.9868054037016663\n",
      "T1 have the importance of -0.9895014066068022\n",
      "RH_1 have the importance of -0.3200427236045851\n",
      "T2 have the importance of -0.06752775581890447\n",
      "RH_2 have the importance of 0.05600440751085678\n",
      "T3 have the importance of 0.24088913229968512\n",
      "RH_3 have the importance of 0.8062264991777734\n",
      "T4 have the importance of 0.0011094876277777388\n",
      "RH_4 have the importance of 0.07393067167042666\n",
      "T5 have the importance of 0.17285651467916985\n",
      "RH_5 have the importance of 1.9215667817714728\n",
      "T6 have the importance of -0.22513077351771216\n",
      "RH_6 have the importance of 0.10933728401362686\n",
      "T7 have the importance of 1.1188694862791628\n",
      "RH_7 have the importance of 0.01125241440978364\n",
      "T8 have the importance of 0.7741434868563033\n",
      "RH_8 have the importance of 0.4741611984193952\n",
      "T9 have the importance of 0.36221104059448983\n",
      "RH_9 have the importance of 0.0\n",
      "T_out have the importance of 0.0689157450452278\n",
      "Press_mm_hg have the importance of 0.5986266086785808\n",
      "RH_out have the importance of 1.5741549787493252\n",
      "Windspeed have the importance of 0.06697385846242021\n",
      "Visibility have the importance of 0.2161607968152115\n",
      "Tdewpoint have the importance of 0.0\n",
      "rv1 have the importance of 0.0\n"
     ]
    }
   ],
   "source": [
    "importances = calculate_All_Feature_Importance(model_RF, X_train, Y_train)\n",
    "for label, score in zip(data.columns, importances):\n",
    "    print(f\"{label} have the importance of {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliances has the highest score followed by RH_5, T6 and RH_8. Therefore,  Appliances, RH_5, T6 and RH_8 are the most important features for Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d.4 GradiantBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appliances have the importance of 278.8108434619976\n",
      "lights have the importance of -25.190488236717556\n",
      "T1 have the importance of 14.160145839144207\n",
      "RH_1 have the importance of 130.5446901852356\n",
      "T2 have the importance of 85.31293066547232\n",
      "RH_2 have the importance of 267.9208600334864\n",
      "T3 have the importance of -190.48000854476186\n",
      "RH_3 have the importance of 21.110476903028484\n",
      "T4 have the importance of 19.952185609416574\n",
      "RH_4 have the importance of 28.698451338666473\n",
      "T5 have the importance of -23.541078299128458\n",
      "RH_5 have the importance of 352.78947228324705\n",
      "T6 have the importance of 371.7941123152086\n",
      "RH_6 have the importance of -9.383296690524276\n",
      "T7 have the importance of 517.981424188004\n",
      "RH_7 have the importance of 129.94173882121186\n",
      "T8 have the importance of 224.22134437545492\n",
      "RH_8 have the importance of -29.281719012983558\n",
      "T9 have the importance of 111.93121885726896\n",
      "RH_9 have the importance of 88.36315787177591\n",
      "T_out have the importance of 77.83386320146155\n",
      "Press_mm_hg have the importance of 528.5360866856672\n",
      "RH_out have the importance of 48.37737197972683\n",
      "Windspeed have the importance of 44.16565111520549\n",
      "Visibility have the importance of 27.756898795022153\n",
      "Tdewpoint have the importance of 0.0\n",
      "rv1 have the importance of -8.585786740232379\n"
     ]
    }
   ],
   "source": [
    "importances = calculate_All_Feature_Importance(model_GradiantBoost, X_train, Y_train)\n",
    "for label, score in zip(data.columns, importances):\n",
    "    print(f\"{label} have the importance of {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here r-square values are negative and will cause very large score of importance after substraction. And therefore the score of rv1 is reasonable. So rv1 is the important feature of GradiantBoost model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "73c0d8c6685d021cc806876c6b57cd6b398f87cd866947b141d36c88c6af2f44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
