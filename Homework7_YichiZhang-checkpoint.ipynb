{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Custom decision tree regressor\n",
    "from statmodels.decisiontrees import DecisionTreeRegressor\n",
    "#Custom random forest regressor\n",
    "from statmodels.random_forest import RandomForestRegressor\n",
    "#Custom lasso regressor\n",
    "from statmodels.regression import LassoRegression\n",
    "#Custom gradient boosting regressor\n",
    "from statmodels.gradientboosting import GradientBoostTreeRegressor\n",
    "\n",
    "#Find performance\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (19735, 27), shape of y: (19735,)\n",
      "Shape of X_Train: (15788, 27), shape of y_Train: (15788,)\n",
      "Shape of X_Train: (3947, 27), shape of y_Train: (3947,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"energydata_complete.csv\")\n",
    "data = data.set_index(\"date\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.iloc[:, 1:].values\n",
    "Y = data.iloc[:, 0].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=.2, random_state=41)\n",
    "print(f\"Shape of X: {X.shape}, shape of y: {Y.shape}\")\n",
    "print(f\"Shape of X_Train: {X_train.shape}, shape of y_Train: {Y_train.shape}\")\n",
    "print(f\"Shape of X_Train: {X_test.shape}, shape of y_Train: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(Y, Y_Hat):\n",
    "    return np.sqrt(mean_squared_error(Y, Y_Hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.1 Lasso regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gqf12\\anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\gqf12\\Documents\\ml\\homework7\\statmodels\\regression\\utils\\metrics.py:52: RuntimeWarning: overflow encountered in square\n",
      "  errors = np.average((y_true - y_pred) ** 2, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<statmodels.regression.regression.LassoRegression at 0x1f854a41340>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=data.columns[1:])\n",
    "model_LassoRegression = LassoRegression(n_iter=1000, lr=1e-3, alpha=0.1)\n",
    "model_LassoRegression.fit(X_train.values, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m y_hat_LassoReg \u001b[38;5;241m=\u001b[39m model_LassoRegression\u001b[38;5;241m.\u001b[39mpredict(X_train\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLasso Regression - Root Mean Squared error:\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m----> 3\u001b[0m       \u001b[38;5;28mround\u001b[39m(\u001b[43mrmse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat_LassoReg\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLasso Regression - R-Squared:\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m       \u001b[38;5;28mround\u001b[39m(r2_score(Y_train, y_hat_LassoReg), \u001b[38;5;241m4\u001b[39m))\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mrmse\u001b[1;34m(Y, Y_Hat)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrmse\u001b[39m(Y, Y_Hat):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_Hat\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\metrics\\_regression.py:442\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_squared_error\u001b[39m(\n\u001b[0;32m    383\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    384\u001b[0m ):\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    446\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\metrics\\_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m    101\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 102\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    105\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    896\u001b[0m         )\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 899\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[0;32m    126\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m estimator_name\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    131\u001b[0m             \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m             msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m             )\n\u001b[1;32m--> 146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "\n",
    "y_hat_LassoReg = model_LassoRegression.predict(X_train.values)\n",
    "print('Lasso Regression - Root Mean Squared error:',\n",
    "      round(rmse(Y_train, y_hat_LassoReg), 4))\n",
    "print('Lasso Regression - R-Squared:',\n",
    "      round(r2_score(Y_train, y_hat_LassoReg), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Root Mean Squared error of Lasso regression model is 97.543. As the lower the RMSE, the better the model, this model does not fit the train dataset well. This model has the R-Squared value of 0.0972, which means 9.72% of the variability observed in the target variable is explained by the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.2 Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeRegressor(max_depth=3, min_samples_split=2)\n",
    "model_tree.fit(X_train_Transformed, Y_train.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree Regression - Root Mean Squared error: 101.1047\n",
      "Decision tree Regression - R-Squared: 0.0301\n"
     ]
    }
   ],
   "source": [
    "y_hat_tree = model_tree.predict(X_train_Transformed.values)\n",
    "print('Decision tree Regression - Root Mean Squared error:', round(rmse(Y_train,y_hat_tree), 4))\n",
    "print('Decision tree Regression - R-Squared:', round(r2_score(Y_train,y_hat_tree), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Root Mean Squared error of Decision tree model is 100.5505. This model does not fit the train dataset well. This model has the R-Squared value of 0.0407, which means 4.07% of the variability observed in the target variable is explained by the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.3 Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestRegressor(n_trees = 5, max_depth = 3, min_samples_split = 5)\n",
    "model_RF.fit(X_train_Transformed, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression - Root Mean Squared error: 99.7805\n",
      "Random Forest Regression - R-Squared: 0.0553\n"
     ]
    }
   ],
   "source": [
    "y_hat_RF = model_RF.predict(X_train_Transformed.values)\n",
    "print('Random Forest Regression - Root Mean Squared error:', round(rmse(Y_train,y_hat_RF), 4))\n",
    "print('Random Forest Regression - R-Squared:', round(r2_score(Y_train,y_hat_RF), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Root Mean Squared error of Random Forest model is 99.6317. This model does not fit the train dataset well. This model has the R-Squared value of 0.0582, which means 5.82% of the variability observed in the target variable is explained by the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.4 GradientBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GradiantBoost = GradientBoostTreeRegressor(n_elements=5, learning_rate=0.01)\n",
    "    \n",
    "#fit the model\n",
    "model_GradiantBoost.fit(np.array(X_train_Transformed), Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Regression - Root Mean Squared error: 137.0879\n",
      "Gradient Boost Regression - R-Squared: -0.7831\n"
     ]
    }
   ],
   "source": [
    "y_hat_GB = model_GradiantBoost.predict(X_train)\n",
    "print('Gradient Boost Regression - Root Mean Squared error:', round(rmse(Y_train,y_hat_GB),4))\n",
    "print('Gradient Boost Regression - R-Squared:', round(r2_score(Y_train, y_hat_GB),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Root Mean Squared error of GradientBoost model is 137.0879. This model does not fit the train dataset well. This model has the R-Squared value of -0.7831, which this model fits the dataset poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "The Lasso regression model has the lowest RMSE, which means it predicts the training data best. And it also has the highest R-square value which means it explains the most variability of the training dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.1 Lasso regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression - Root Mean Squared error: 97.1061\n",
      "Lasso Regression - R-Squared: 0.093\n"
     ]
    }
   ],
   "source": [
    "y_hat_test_LassoReg = model_LassoRegression.predict(X_test)\n",
    "print('Lasso Regression - Root Mean Squared error:',\n",
    "      round(rmse(Y_test, y_hat_test_LassoReg), 4))\n",
    "print('Lasso Regression - R-Squared:',\n",
    "      round(r2_score(Y_test, y_hat_test_LassoReg), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression has RMSE of 195.5506 and R-Squared value of -2.6782. This means it does not predict the test dataset well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.2 Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree Regression - Root Mean Squared error: 101.436\n",
      "Decision tree Regression - R-Squared: 0.0103\n"
     ]
    }
   ],
   "source": [
    "y_hat_test_tree = model_tree.predict(X_test)\n",
    "print('Decision tree Regression - Root Mean Squared error:', round(rmse(Y_test, y_hat_test_tree), 4))\n",
    "print('Decision tree Regression - R-Squared:', round(r2_score(Y_test, y_hat_test_tree), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree has RMSE of 151.6432 and R-Squared value of -1.2119. This means it does not predict the test dataset well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.3 Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression - Root Mean Squared error: 99.4681\n",
      "Random Forest Regression - R-Squared: 0.0483\n"
     ]
    }
   ],
   "source": [
    "y_hat_test_RF = model_RF.predict(X_test)\n",
    "print('Random Forest Regression - Root Mean Squared error:', round(rmse(Y_test, y_hat_test_RF.reshape(-1)), 4))\n",
    "print('Random Forest Regression - R-Squared:', round(r2_score(Y_test, y_hat_test_RF.reshape(-1)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree has RMSE of 100.8583 and R-Squared value of 0.0215. This model predict well on the test dataset. And 2.15% of the variability observed in the target variable is explained by the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.4 GradientBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Regression - Root Mean Squared error: 137.1734\n",
      "Gradient Boost Regression - R-Squared: -0.8099\n"
     ]
    }
   ],
   "source": [
    "y_hat_test_GB = model_GradiantBoost.predict(X_test)\n",
    "print('Gradient Boost Regression - Root Mean Squared error:', round(rmse(Y_test, y_hat_test_GB),4))\n",
    "print('Gradient Boost Regression - R-Squared:', round(r2_score(Y_test,  y_hat_test_GB),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoost model has RMSE of 16594.1495 and R-Squared value of -0.5962. This means it does not predict the test dataset well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_after_permutation(model, X, y, col_idx):\n",
    "    X_permuted = X.copy()\n",
    "    X_permuted[:, col_idx] = np.random.permutation(\n",
    "        X_permuted[:, col_idx])\n",
    "    permuted_score = r2_score(model.predict(X_permuted), y.reshape(-1))\n",
    "    return permuted_score\n",
    "\n",
    "\n",
    "def get_feature_importance(model, X, y, col_idx):\n",
    "    baseline_score_train = r2_score(model.predict(X), y.reshape(-1))\n",
    "    permuted_score_train = get_score_after_permutation(model, X, y, col_idx)\n",
    "    feature_importance = baseline_score_train - permuted_score_train\n",
    "    return feature_importance\n",
    "\n",
    "\n",
    "def calculate_All_Feature_Importance(model, X, y):\n",
    "    list_Feature_Importance = []\n",
    "    for col_Index in range(X.shape[1]):\n",
    "        list_Feature_Importance.append(\n",
    "            get_feature_importance(model, X, y, col_Index))\n",
    "    return list_Feature_Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (15788, 28), indices imply (15788, 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m importances \u001b[39m=\u001b[39m calculate_All_Feature_Importance(model_LassoRegression, X_train_Transformed, Y_train)\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m label, score \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(data\u001b[39m.\u001b[39mcolumns, importances):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m have the importance of \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [17], line 20\u001b[0m, in \u001b[0;36mcalculate_All_Feature_Importance\u001b[1;34m(model, X, y)\u001b[0m\n\u001b[0;32m     17\u001b[0m list_Feature_Importance \u001b[39m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m col_Index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[0;32m     19\u001b[0m     list_Feature_Importance\u001b[39m.\u001b[39mappend(\n\u001b[1;32m---> 20\u001b[0m         get_feature_importance(model, X, y, col_Index))\n\u001b[0;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m list_Feature_Importance\n",
      "Cell \u001b[1;32mIn [17], line 10\u001b[0m, in \u001b[0;36mget_feature_importance\u001b[1;34m(model, X, y, col_idx)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_feature_importance\u001b[39m(model, X, y, col_idx):\n\u001b[1;32m---> 10\u001b[0m     baseline_score_train \u001b[39m=\u001b[39m r2_score(model\u001b[39m.\u001b[39;49mpredict(X), y\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     11\u001b[0m     permuted_score_train \u001b[39m=\u001b[39m get_score_after_permutation(model, X, y, col_idx)\n\u001b[0;32m     12\u001b[0m     feature_importance \u001b[39m=\u001b[39m baseline_score_train \u001b[39m-\u001b[39m permuted_score_train\n",
      "File \u001b[1;32mc:\\Users\\gqf12\\Documents\\ml\\homework7\\statmodels\\regression\\regression.py:95\u001b[0m, in \u001b[0;36mRegression.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39mEstimate target values using the linear model.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39m    Estimated targets per instance.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39m# Insert X_0 = 1 for the bias term.\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49minsert(X, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m X\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minsert\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gqf12\\anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\lib\\function_base.py:5357\u001b[0m, in \u001b[0;36minsert\u001b[1;34m(arr, obj, values, axis)\u001b[0m\n\u001b[0;32m   5355\u001b[0m     new[\u001b[39mtuple\u001b[39m(slobj)] \u001b[39m=\u001b[39m arr[\u001b[39mtuple\u001b[39m(slobj2)]\n\u001b[0;32m   5356\u001b[0m     \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m-> 5357\u001b[0m         \u001b[39mreturn\u001b[39;00m wrap(new)\n\u001b[0;32m   5358\u001b[0m     \u001b[39mreturn\u001b[39;00m new\n\u001b[0;32m   5359\u001b[0m \u001b[39melif\u001b[39;00m indices\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m   5360\u001b[0m     \u001b[39m# Can safely cast the empty list to intp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gqf12\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\generic.py:2107\u001b[0m, in \u001b[0;36mNDFrame.__array_wrap__\u001b[1;34m(self, result, context)\u001b[0m\n\u001b[0;32m   2105\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m   2106\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_axes_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_AXIS_ORDERS, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 2107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(res, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39md)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__array_wrap__\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gqf12\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\frame.py:720\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    710\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    711\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    712\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    717\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    718\u001b[0m         )\n\u001b[0;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    721\u001b[0m             data,\n\u001b[0;32m    722\u001b[0m             index,\n\u001b[0;32m    723\u001b[0m             columns,\n\u001b[0;32m    724\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    725\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    726\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    727\u001b[0m         )\n\u001b[0;32m    729\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\gqf12\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\internals\\construction.py:349\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    345\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[0;32m    346\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[0;32m    347\u001b[0m )\n\u001b[1;32m--> 349\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\gqf12\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (15788, 28), indices imply (15788, 27)"
     ]
    }
   ],
   "source": [
    "importances = calculate_All_Feature_Importance(model_LassoRegression, X_train_Transformed, Y_train)\n",
    "for label, score in zip(data.columns, importances):\n",
    "    print(f\"{label} have the importance of {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = calculate_All_Feature_Importance(model_tree, X_train_Transformed, Y_train)\n",
    "for label, score in zip(data.columns, importances):\n",
    "    print(f\"{label} have the importance of {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = calculate_All_Feature_Importance(model_RF, X_train_Transformed, Y_train)\n",
    "for label, score in zip(data.columns, importances):\n",
    "    print(f\"{label} have the importance of {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = calculate_All_Feature_Importance(model_GradiantBoost, X_train_Transformed, Y_train)\n",
    "for label, score in zip(data.columns, importances):\n",
    "    print(f\"{label} have the importance of {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e057365759ce64748f45be5b74d3c1d08d296b4c0e8363ba1822990142ab9c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
